{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5bacd34f-5010-4b1d-b75d-cc1286b429e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e7d4cd4-fefd-4b10-9897-337025595cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "clean_path = '4000_0.05_clean.csv'\n",
    "dirty_path = '4000_0.05_dirty.csv'\n",
    "\n",
    "clean_df = pd.read_csv(clean_path)\n",
    "dirty_df = pd.read_csv(dirty_path)\n",
    "\n",
    "# Display basic info about the datasets\n",
    "# print('Clean Data Info:')\n",
    "# print(clean_df.info())\n",
    "# print('\\nDirty Data Info:')\n",
    "# print(dirty_df.info())\n",
    "\n",
    "# Check the first few rows of the datasets\n",
    "# print('\\nClean Data Sample:')\n",
    "# print(clean_df.head())\n",
    "# print('\\nDirty Data Sample:')\n",
    "# print(dirty_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7388575a-297c-4739-9be3-975efb6dae21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare Data: target variable y is the binary classification of NATIVITY (whether a person is Native or Foreign-born)\n",
    "# The sensitive attribute RAC1P (likely race or ethnicity) is separated out to evaluate fairness\n",
    "def prepare_data(df, is_clean=True):\n",
    "    if is_clean:\n",
    "        df = df.iloc[:, :-8] # Dropping the last 8 columns of the clean dataset\n",
    "    X = df.drop(['NATIVITY', 'RAC1P'], axis=1)\n",
    "    y = (df['NATIVITY'] == 1).astype(int) # Binary classification: 1 if Native, 0 if Foreign-born\n",
    "    sensitive_attr = df['RAC1P']\n",
    "    X_train, X_test, y_train, y_test, s_train, s_test = train_test_split(X, y, sensitive_attr, test_size=0.2, random_state=42)\n",
    "    return X_train, X_test, y_train, y_test, s_train, s_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97684a00-aaa5-428b-addc-a2b6dbbfdd0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare training and testing sets: data is split into training and testing sets, along with the sensitive attribute.\n",
    "X_train_clean, X_test_clean, y_train_clean, y_test_clean, s_train_clean, s_test_clean = prepare_data(clean_df, is_clean=True)\n",
    "X_train_dirty, X_test_dirty, y_train_dirty, y_test_dirty, s_train_dirty, s_test_dirty = prepare_data(dirty_df, is_clean=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "313eabec-746e-4e4e-8f50-db9b67d65bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fairness Metrics\n",
    "def demographic_parity(y_pred, sensitive_attr): # Measures whether the prediction rates are equal across different race groups\n",
    "    groups = sensitive_attr.unique()\n",
    "    rates = {group: (y_pred[sensitive_attr == group].mean()) for group in groups}\n",
    "    return rates\n",
    "\n",
    "def equalized_odds(y_true, y_pred, sensitive_attr): # Measures whether true positive rates and false positive rates are equal across race groups\n",
    "    groups = sensitive_attr.unique()\n",
    "    true_positive_rates = {}\n",
    "    false_positive_rates = {}\n",
    "\n",
    "    for group in groups:\n",
    "        y_true_group = y_true[sensitive_attr == group]\n",
    "        y_pred_group = y_pred[sensitive_attr == group]\n",
    "        \n",
    "        tp = ((y_pred_group == 1) & (y_true_group == 1)).sum()\n",
    "        tn = ((y_pred_group == 0) & (y_true_group == 0)).sum()\n",
    "        fp = ((y_pred_group == 1) & (y_true_group == 0)).sum()\n",
    "        fn = ((y_pred_group == 0) & (y_true_group == 1)).sum()\n",
    "        \n",
    "        tpr = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        fpr = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
    "        \n",
    "        true_positive_rates[group] = tpr\n",
    "        false_positive_rates[group] = fpr\n",
    "\n",
    "    return true_positive_rates, false_positive_rates\n",
    "\n",
    "def disparate_impact_ratio(y_pred, sensitive_attr): # Ratio of the minimum prediction rate to the maximum prediction rate across groups\n",
    "    groups = sensitive_attr.unique()\n",
    "    rates = {group: (y_pred[sensitive_attr == group].mean()) for group in groups}\n",
    "    min_rate = min(rates.values())\n",
    "    max_rate = max(rates.values())\n",
    "    return min_rate / max_rate if max_rate > 0 else 0\n",
    "\n",
    "def statistical_parity_difference(y_pred, sensitive_attr): # Difference between the maximum and minimum prediction rates across groups\n",
    "    groups = sensitive_attr.unique()\n",
    "    rates = {group: (y_pred[sensitive_attr == group].mean()) for group in groups}\n",
    "    max_rate = max(rates.values())\n",
    "    min_rate = min(rates.values())\n",
    "    return max_rate - min_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "34c729c5-279c-4527-8f6c-9fe92b36f6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_train_clean_test_clean = LogisticRegression(max_iter=1000)\n",
    "logreg_train_clean_test_clean.fit(X_train_clean, y_train_clean)\n",
    "y_pred_logreg_train_clean_test_clean = logreg_train_clean_test_clean.predict(X_test_clean)\n",
    "\n",
    "logreg_train_clean_test_dirty = LogisticRegression(max_iter=1000)\n",
    "logreg_train_clean_test_dirty.fit(X_train_clean, y_train_clean)\n",
    "y_pred_logreg_train_clean_test_dirty = logreg_train_clean_test_dirty.predict(X_test_dirty)\n",
    "\n",
    "logreg_train_dirty_test_clean = LogisticRegression(max_iter=1000)\n",
    "logreg_train_dirty_test_clean.fit(X_train_dirty, y_train_dirty)\n",
    "y_pred_logreg_train_dirty_test_clean = logreg_train_dirty_test_clean.predict(X_test_clean)\n",
    "\n",
    "logreg_train_dirty_test_dirty = LogisticRegression(max_iter=1000)\n",
    "logreg_train_dirty_test_dirty.fit(X_train_dirty, y_train_dirty)\n",
    "y_pred_logreg_train_dirty_test_dirty = logreg_train_dirty_test_dirty.predict(X_test_dirty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "879f76c6-0611-40e7-9096-8a0c6d9c1620",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_train_clean_test_clean = DecisionTreeClassifier()\n",
    "tree_train_clean_test_clean.fit(X_train_clean, y_train_clean)\n",
    "y_pred_tree_train_clean_test_clean = tree_train_clean_test_clean.predict(X_test_clean)\n",
    "\n",
    "tree_train_clean_test_dirty = DecisionTreeClassifier()\n",
    "tree_train_clean_test_dirty.fit(X_train_clean, y_train_clean)\n",
    "y_pred_tree_train_clean_test_dirty = tree_train_clean_test_dirty.predict(X_test_dirty)\n",
    "\n",
    "tree_train_dirty_test_clean = DecisionTreeClassifier()\n",
    "tree_train_dirty_test_clean.fit(X_train_dirty, y_train_dirty)\n",
    "y_pred_tree_train_dirty_test_clean = tree_train_dirty_test_clean.predict(X_test_clean)\n",
    "\n",
    "tree_train_dirty_test_dirty = DecisionTreeClassifier()\n",
    "tree_train_dirty_test_dirty.fit(X_train_dirty, y_train_dirty)\n",
    "y_pred_tree_train_dirty_test_dirty = tree_train_dirty_test_dirty.predict(X_test_dirty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a187123d-cff4-44a6-8345-4b3504974a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_train_clean_test_clean = RandomForestClassifier()\n",
    "rf_train_clean_test_clean.fit(X_train_clean, y_train_clean)\n",
    "y_pred_rf_train_clean_test_clean = rf_train_clean_test_clean.predict(X_test_clean)\n",
    "\n",
    "rf_train_clean_test_dirty = RandomForestClassifier()\n",
    "rf_train_clean_test_dirty.fit(X_train_clean, y_train_clean)\n",
    "y_pred_rf_train_clean_test_dirty = rf_train_clean_test_dirty.predict(X_test_dirty)\n",
    "\n",
    "rf_train_dirty_test_clean = RandomForestClassifier()\n",
    "rf_train_dirty_test_clean.fit(X_train_dirty, y_train_dirty)\n",
    "y_pred_rf_train_dirty_test_clean = rf_train_dirty_test_clean.predict(X_test_clean)\n",
    "\n",
    "rf_train_dirty_test_dirty = RandomForestClassifier()\n",
    "rf_train_dirty_test_dirty.fit(X_train_dirty, y_train_dirty)\n",
    "y_pred_rf_train_dirty_test_dirty = rf_train_dirty_test_dirty.predict(X_test_dirty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "432f76fb-19a4-4773-af25-eaf10cecbe3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_train_clean_test_clean = SVC()\n",
    "svm_train_clean_test_clean.fit(X_train_clean, y_train_clean)\n",
    "y_pred_svm_train_clean_test_clean = svm_train_clean_test_clean.predict(X_test_clean)\n",
    "\n",
    "svm_train_clean_test_dirty = SVC()\n",
    "svm_train_clean_test_dirty.fit(X_train_clean, y_train_clean)\n",
    "y_pred_svm_train_clean_test_dirty = svm_train_clean_test_dirty.predict(X_test_dirty)\n",
    "\n",
    "svm_train_dirty_test_clean = SVC()\n",
    "svm_train_dirty_test_clean.fit(X_train_dirty, y_train_dirty)\n",
    "y_pred_svm_train_dirty_test_clean = svm_train_dirty_test_clean.predict(X_test_clean)\n",
    "\n",
    "svm_train_dirty_test_dirty = SVC()\n",
    "svm_train_dirty_test_dirty.fit(X_train_dirty, y_train_dirty)\n",
    "y_pred_svm_train_dirty_test_dirty = svm_train_dirty_test_dirty.predict(X_test_dirty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8f31d4bf-a8cc-42da-ae9a-56a3f65ac905",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy (Train: Clean, Test: Clean): 0.99875\n",
      "Logistic Regression Accuracy (Train: Clean, Test: Dirty): 0.9125\n",
      "Logistic Regression Accuracy (Train: Dirty, Test: Clean): 0.99375\n",
      "Logistic Regression Accuracy (Train: Dirty, Test: Dirty): 0.93125\n"
     ]
    }
   ],
   "source": [
    "print('Logistic Regression Accuracy (Train: Clean, Test: Clean):', accuracy_score(y_test_clean, y_pred_logreg_train_clean_test_clean))\n",
    "print('Logistic Regression Accuracy (Train: Clean, Test: Dirty):', accuracy_score(y_test_dirty, y_pred_logreg_train_clean_test_dirty))\n",
    "print('Logistic Regression Accuracy (Train: Dirty, Test: Clean):', accuracy_score(y_test_clean, y_pred_logreg_train_dirty_test_clean))\n",
    "print('Logistic Regression Accuracy (Train: Dirty, Test: Dirty):', accuracy_score(y_test_dirty, y_pred_logreg_train_dirty_test_dirty))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9b076cb4-3005-4714-ba61-24cb462c2481",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Accuracy (Train: Clean, Test: Clean): 1.0\n",
      "Decision Tree Accuracy (Train: Clean, Test: Dirty): 0.93\n",
      "Decision Tree Accuracy (Train: Dirty, Test: Clean): 0.945\n",
      "Decision Tree Accuracy (Train: Dirty, Test: Dirty): 0.895\n"
     ]
    }
   ],
   "source": [
    "print('Decision Tree Accuracy (Train: Clean, Test: Clean):', accuracy_score(y_test_clean, y_pred_tree_train_clean_test_clean))\n",
    "print('Decision Tree Accuracy (Train: Clean, Test: Dirty):', accuracy_score(y_test_dirty, y_pred_tree_train_clean_test_dirty))\n",
    "print('Decision Tree Accuracy (Train: Dirty, Test: Clean):', accuracy_score(y_test_clean, y_pred_tree_train_dirty_test_clean))\n",
    "print('Decision Tree Accuracy (Train: Dirty, Test: Dirty):', accuracy_score(y_test_dirty, y_pred_tree_train_dirty_test_dirty))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a4b6959e-de0b-40ba-a9ee-311e7c5f1d70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Accuracy (Train: Clean, Test: Clean): 1.0\n",
      "Random Forest Accuracy (Train: Clean, Test: Dirty): 0.93125\n",
      "Random Forest Accuracy (Train: Dirty, Test: Clean): 0.9875\n",
      "Random Forest Accuracy (Train: Dirty, Test: Dirty): 0.9375\n"
     ]
    }
   ],
   "source": [
    "print('Random Forest Accuracy (Train: Clean, Test: Clean):', accuracy_score(y_test_clean, y_pred_rf_train_clean_test_clean))\n",
    "print('Random Forest Accuracy (Train: Clean, Test: Dirty):', accuracy_score(y_test_dirty, y_pred_rf_train_clean_test_dirty))\n",
    "print('Random Forest Accuracy (Train: Dirty, Test: Clean):', accuracy_score(y_test_clean, y_pred_rf_train_dirty_test_clean))\n",
    "print('Random Forest Accuracy (Train: Dirty, Test: Dirty):', accuracy_score(y_test_dirty, y_pred_rf_train_dirty_test_dirty))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "18a22e88-a936-4491-be8e-7c7ed4bf6cda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Accuracy (Train: Clean, Test: Clean): 0.85\n",
      "SVM Accuracy (Train: Clean, Test: Dirty): 0.84125\n",
      "SVM Accuracy (Train: Dirty, Test: Clean): 0.85\n",
      "SVM Accuracy (Train: Dirty, Test: Dirty): 0.84125\n"
     ]
    }
   ],
   "source": [
    "print('SVM Accuracy (Train: Clean, Test: Clean):', accuracy_score(y_test_clean, y_pred_svm_train_clean_test_clean))\n",
    "print('SVM Accuracy (Train: Clean, Test: Dirty):', accuracy_score(y_test_dirty, y_pred_svm_train_clean_test_dirty))\n",
    "print('SVM Accuracy (Train: Dirty, Test: Clean):', accuracy_score(y_test_clean, y_pred_svm_train_dirty_test_clean))\n",
    "print('SVM Accuracy (Train: Dirty, Test: Dirty):', accuracy_score(y_test_dirty, y_pred_svm_train_dirty_test_dirty))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd674f13-6f00-4429-aea7-fa9f62844086",
   "metadata": {},
   "source": [
    "### Four Scenarios\n",
    "\n",
    "Train: Clean, Test: Clean - best-case benchmark, model sees only high-integrity data\n",
    "Train: Clean, Test: Dirty - measures how robust a model trained on clean data is to real-world bias or corrupted inputs\n",
    "Train: Dirty, Test: Clean - evaluates if a model trained on biased data still performs well on unbiased data (insight into bias internalization)\n",
    "Train: Dirty, Test: Dirty - realistic worst-case for fairness, data is perturbed throughout, helps us understand what users would experience under biased conditions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac4efc0c-3710-40af-badc-9a4a59a04b34",
   "metadata": {},
   "source": [
    "### Model Insights\n",
    "\n",
    "Logistic regression is highly sensitive to perturbations in the test data, dropping from 99.9% to 91.3% when tested on dirty data. But it still performs reasonably well when trained on dirty data. This indicates that the model isn’t overfitting to spurious correlations, but it does suffer when test data deviates from training.\n",
    "\n",
    "Decision Trees are slightly more brittle. The performance drop is sharper across the board, suggesting that trees are more likely to learn (and depend on) sensitive attribute relationships that get disrupted in dirty data. They don't generalize as well under perturbation.\n",
    "\n",
    "Ensembles like Random Forests are more robust than single trees. While there is still a drop from Clean to Dirty, it is less dramatic. They handle noise and biased training slightly better, thanks to averaging over many trees. Best balance of generalization and accuracy.\n",
    "\n",
    "SVMs have stable, but lower performance. Interestingly, they’re less sensitive to the dirty/clean switch, but never reach high accuracy. This may indicate underfitting, or that the feature space isn’t separable with a linear kernel (which is the default unless otherwise specified)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5bf15aac-c615-4f5c-9639-d78fcf418815",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fairness Metrics for Logistic Regression models\n",
    "\n",
    "# print(\"Train: Clean, Test: Clean\")\n",
    "# print(\"Demographic Parity:\", demographic_parity(y_pred_logreg_train_clean_test_clean, s_test_clean))\n",
    "# print(\"Equalized Odds:\", equalized_odds(y_test_clean, y_pred_logreg_train_clean_test_clean, s_test_clean))\n",
    "# print(\"Disparate Impact Ratio:\", disparate_impact_ratio(y_pred_logreg_train_clean_test_clean, s_test_clean))\n",
    "# print(\"Statistical Parity Difference:\", statistical_parity_difference(y_pred_logreg_train_clean_test_clean, s_test_clean))\n",
    "\n",
    "# print(\"Train: Clean, Test: Dirty\")\n",
    "# print(\"Demographic Parity:\", demographic_parity(y_pred_logreg_train_clean_test_dirty, s_test_dirty))\n",
    "# print(\"Equalized Odds:\", equalized_odds(y_test_dirty, y_pred_logreg_train_clean_test_dirty, s_test_dirty))\n",
    "# print(\"Disparate Impact Ratio:\", disparate_impact_ratio(y_pred_logreg_train_clean_test_dirty, s_test_dirty))\n",
    "# print(\"Statistical Parity Difference:\", statistical_parity_difference(y_pred_logreg_train_clean_test_dirty, s_test_dirty))\n",
    "\n",
    "# print(\"Train: Dirty, Test: Clean\")\n",
    "# print(\"Demographic Parity:\", demographic_parity(y_pred_logreg_train_dirty_test_clean, s_test_clean))\n",
    "# print(\"Equalized Odds:\", equalized_odds(y_test_clean, y_pred_logreg_train_dirty_test_clean, s_test_clean))\n",
    "# print(\"Disparate Impact Ratio:\", disparate_impact_ratio(y_pred_logreg_train_dirty_test_clean, s_test_clean))\n",
    "# print(\"Statistical Parity Difference:\", statistical_parity_difference(y_pred_logreg_train_dirty_test_clean, s_test_clean))\n",
    "\n",
    "# print(\"Train: Dirty, Test: Dirty\")\n",
    "# print(\"Demographic Parity:\", demographic_parity(y_pred_logreg_train_dirty_test_dirty, s_test_dirty))\n",
    "# print(\"Equalized Odds:\", equalized_odds(y_test_dirty, y_pred_logreg_train_dirty_test_dirty, s_test_dirty))\n",
    "# print(\"Disparate Impact Ratio:\", disparate_impact_ratio(y_pred_logreg_train_dirty_test_dirty, s_test_dirty))\n",
    "# print(\"Statistical Parity Difference:\", statistical_parity_difference(y_pred_logreg_train_dirty_test_dirty, s_test_dirty))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "475cde85-db0f-4056-86a0-995e487fa40a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train/Test     Fairness Metric               Value\n",
      "---------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Clean/Clean    Demographic Parity            {1: 0.9519, 9: 0.7531, 2: 0.8904, 8: 0.6200, 6: 0.2769, 7: 1.0000, 3: 1.0000, 5: 0.5000}\n",
      "Clean/Clean    Equalized Odds                ({1: 1.0000, 9: 1.0000, 2: 1.0000, 8: 0.9688, 6: 1.0000, 7: 1.0000, 3: 1.0000, 5: 1.0000}, {1: 0.0000, 9: 0.0000, 2: 0.0000, 8: 0.0000, 6: 0.0000, 7: 0, 3: 0, 5: 0.0000})\n",
      "Clean/Clean    Disparate Impact Ratio        0.2769\n",
      "Clean/Clean    Statistical Parity Difference 0.7231\n",
      "Clean/Dirty    Demographic Parity            {3: 0.7778, 1: 0.9329, 9: 0.7531, 2: 0.8608, 8: 0.6842, 6: 0.4444, 7: 0.7500, 5: 0.7500, 4: 0.5000}\n",
      "Clean/Dirty    Equalized Odds                ({3: 0.8750, 1: 0.9632, 9: 0.9077, 2: 0.9683, 8: 0.9250, 6: 0.8889, 7: 1.0000, 5: 1.0000, 4: 0.6000}, {3: 0.0000, 1: 0.4667, 9: 0.1250, 2: 0.4375, 8: 0.1176, 6: 0.2222, 7: 0.3333, 5: 0.4000, 4: 0.3333})\n",
      "Clean/Dirty    Disparate Impact Ratio        0.4764\n",
      "Clean/Dirty    Statistical Parity Difference 0.4885\n",
      "Dirty/Clean    Demographic Parity            {1: 0.9558, 9: 0.7531, 2: 0.8904, 8: 0.6400, 6: 0.2923, 7: 1.0000, 3: 1.0000, 5: 0.5000}\n",
      "Dirty/Clean    Equalized Odds                ({1: 1.0000, 9: 1.0000, 2: 1.0000, 8: 0.9688, 6: 1.0000, 7: 1.0000, 3: 1.0000, 5: 1.0000}, {1: 0.0800, 9: 0.0000, 2: 0.0000, 8: 0.0556, 6: 0.0213, 7: 0, 3: 0, 5: 0.0000})\n",
      "Dirty/Clean    Disparate Impact Ratio        0.2923\n",
      "Dirty/Clean    Statistical Parity Difference 0.7077\n",
      "Dirty/Dirty    Demographic Parity            {3: 0.7778, 1: 0.9634, 9: 0.7778, 2: 0.8734, 8: 0.6842, 6: 0.4259, 7: 0.7500, 5: 0.8333, 4: 0.3750}\n",
      "Dirty/Dirty    Equalized Odds                ({3: 0.8750, 1: 0.9935, 9: 0.9231, 2: 0.9841, 8: 0.9000, 6: 0.9444, 7: 1.0000, 5: 1.0000, 4: 0.6000}, {3: 0.0000, 1: 0.5000, 9: 0.1875, 2: 0.4375, 8: 0.1765, 6: 0.1667, 7: 0.3333, 5: 0.6000, 4: 0.0000})\n",
      "Dirty/Dirty    Disparate Impact Ratio        0.3892\n",
      "Dirty/Dirty    Statistical Parity Difference 0.5884\n"
     ]
    }
   ],
   "source": [
    "# Fairness Metrics for Logistic Regression\n",
    "\n",
    "results = {\n",
    "    \"Clean/Clean\": {\n",
    "        \"Demographic Parity\": demographic_parity(y_pred_logreg_train_clean_test_clean, s_test_clean),\n",
    "        \"Equalized Odds\": equalized_odds(y_test_clean, y_pred_logreg_train_clean_test_clean, s_test_clean),\n",
    "        \"Disparate Impact Ratio\": disparate_impact_ratio(y_pred_logreg_train_clean_test_clean, s_test_clean),\n",
    "        \"Statistical Parity Difference\": statistical_parity_difference(y_pred_logreg_train_clean_test_clean, s_test_clean)\n",
    "    },\n",
    "    \"Clean/Dirty\": {\n",
    "        \"Demographic Parity\": demographic_parity(y_pred_logreg_train_clean_test_dirty, s_test_dirty),\n",
    "        \"Equalized Odds\": equalized_odds(y_test_dirty, y_pred_logreg_train_clean_test_dirty, s_test_dirty),\n",
    "        \"Disparate Impact Ratio\": disparate_impact_ratio(y_pred_logreg_train_clean_test_dirty, s_test_dirty),\n",
    "        \"Statistical Parity Difference\": statistical_parity_difference(y_pred_logreg_train_clean_test_dirty, s_test_dirty)\n",
    "    },\n",
    "    \"Dirty/Clean\": {\n",
    "        \"Demographic Parity\": demographic_parity(y_pred_logreg_train_dirty_test_clean, s_test_clean),\n",
    "        \"Equalized Odds\": equalized_odds(y_test_clean, y_pred_logreg_train_dirty_test_clean, s_test_clean),\n",
    "        \"Disparate Impact Ratio\": disparate_impact_ratio(y_pred_logreg_train_dirty_test_clean, s_test_clean),\n",
    "        \"Statistical Parity Difference\": statistical_parity_difference(y_pred_logreg_train_dirty_test_clean, s_test_clean)\n",
    "    },\n",
    "    \"Dirty/Dirty\": {\n",
    "        \"Demographic Parity\": demographic_parity(y_pred_logreg_train_dirty_test_dirty, s_test_dirty),\n",
    "        \"Equalized Odds\": equalized_odds(y_test_dirty, y_pred_logreg_train_dirty_test_dirty, s_test_dirty),\n",
    "        \"Disparate Impact Ratio\": disparate_impact_ratio(y_pred_logreg_train_dirty_test_dirty, s_test_dirty),\n",
    "        \"Statistical Parity Difference\": statistical_parity_difference(y_pred_logreg_train_dirty_test_dirty, s_test_dirty)\n",
    "    }\n",
    "}\n",
    "\n",
    "col_widths = {\n",
    "    \"Train/Test\": 15,\n",
    "    \"Fairness Metric\": 30\n",
    "}\n",
    "\n",
    "# Header\n",
    "print(f\"{'Train/Test':<{col_widths['Train/Test']}}\"\n",
    "      f\"{'Fairness Metric':<{col_widths['Fairness Metric']}}\"\n",
    "      f\"Value\")\n",
    "print(\"-\" * (col_widths['Train/Test'] + col_widths['Fairness Metric'] + 96))\n",
    "\n",
    "def format_value(val):\n",
    "    if isinstance(val, float):\n",
    "        return f\"{val:.4f}\"\n",
    "    elif isinstance(val, dict):\n",
    "        return \"{\" + \", \".join(f\"{k}: {v:.4f}\" if isinstance(v, float) else f\"{k}: {v}\" for k, v in val.items()) + \"}\"\n",
    "    elif isinstance(val, tuple):\n",
    "        # Handle tuple of dicts or other types\n",
    "        return \"(\" + \", \".join(\n",
    "            \"{\" + \", \".join(f\"{k}: {v:.4f}\" if isinstance(v, float) else f\"{k}: {v}\" for k, v in d.items()) + \"}\" \n",
    "            if isinstance(d, dict) else \n",
    "            (f\"{d:.4f}\" if isinstance(d, float) else str(d))\n",
    "            for d in val\n",
    "        ) + \")\"\n",
    "    else:\n",
    "        return str(val)\n",
    "\n",
    "# Rows\n",
    "for config, metrics in results.items():\n",
    "    for metric_name, metric_value in metrics.items():\n",
    "        formatted = format_value(metric_value)\n",
    "        print(f\"{config:<{col_widths['Train/Test']}}\"\n",
    "              f\"{metric_name:<{col_widths['Fairness Metric']}}\"\n",
    "              f\"{formatted}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "570763e9-443a-4a50-bf72-dcad6c69ff3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train/Test     Fairness Metric               Value\n",
      "---------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Clean/Clean    Demographic Parity            {1: 0.9519, 9: 0.7531, 2: 0.8904, 8: 0.6400, 6: 0.2769, 7: 1.0000, 3: 1.0000, 5: 0.5000}\n",
      "Clean/Clean    Equalized Odds                ({1: 1.0000, 9: 1.0000, 2: 1.0000, 8: 1.0000, 6: 1.0000, 7: 1.0000, 3: 1.0000, 5: 1.0000}, {1: 0.0000, 9: 0.0000, 2: 0.0000, 8: 0.0000, 6: 0.0000, 7: 0, 3: 0, 5: 0.0000})\n",
      "Clean/Clean    Disparate Impact Ratio        0.2769\n",
      "Clean/Clean    Statistical Parity Difference 0.7231\n",
      "Clean/Dirty    Demographic Parity            {3: 0.8889, 1: 0.9390, 9: 0.7654, 2: 0.8354, 8: 0.7018, 6: 0.3889, 7: 0.7500, 5: 0.7500, 4: 0.3750}\n",
      "Clean/Dirty    Equalized Odds                ({3: 1.0000, 1: 0.9719, 9: 0.9077, 2: 0.9683, 8: 0.9500, 6: 0.9444, 7: 1.0000, 5: 1.0000, 4: 0.6000}, {3: 0.0000, 1: 0.4333, 9: 0.1875, 2: 0.3125, 8: 0.1176, 6: 0.1111, 7: 0.3333, 5: 0.4000, 4: 0.0000})\n",
      "Clean/Dirty    Disparate Impact Ratio        0.3994\n",
      "Clean/Dirty    Statistical Parity Difference 0.5640\n",
      "Dirty/Clean    Demographic Parity            {1: 0.9346, 9: 0.7654, 2: 0.9178, 8: 0.6800, 6: 0.3538, 7: 1.0000, 3: 0.8000, 5: 0.5000}\n",
      "Dirty/Clean    Equalized Odds                ({1: 0.9737, 9: 0.9180, 2: 0.9538, 8: 1.0000, 6: 1.0000, 7: 1.0000, 3: 0.8000, 5: 1.0000}, {1: 0.1600, 9: 0.3000, 2: 0.6250, 8: 0.1111, 6: 0.1064, 7: 0, 3: 0, 5: 0.0000})\n",
      "Dirty/Clean    Disparate Impact Ratio        0.3538\n",
      "Dirty/Clean    Statistical Parity Difference 0.6462\n",
      "Dirty/Dirty    Demographic Parity            {3: 0.7778, 1: 0.9268, 9: 0.7654, 2: 0.8734, 8: 0.7368, 6: 0.4074, 7: 0.7500, 5: 0.7500, 4: 0.5000}\n",
      "Dirty/Dirty    Equalized Odds                ({3: 0.8750, 1: 0.9545, 9: 0.8615, 2: 0.9206, 8: 0.9500, 6: 0.9444, 7: 1.0000, 5: 1.0000, 4: 0.8000}, {3: 0.0000, 1: 0.5000, 9: 0.3750, 2: 0.6875, 8: 0.2353, 6: 0.1389, 7: 0.3333, 5: 0.4000, 4: 0.0000})\n",
      "Dirty/Dirty    Disparate Impact Ratio        0.4396\n",
      "Dirty/Dirty    Statistical Parity Difference 0.5194\n"
     ]
    }
   ],
   "source": [
    "# Fairness Metrics for Decision Trees\n",
    "\n",
    "results = {\n",
    "    \"Clean/Clean\": {\n",
    "        \"Demographic Parity\": demographic_parity(y_pred_tree_train_clean_test_clean, s_test_clean),\n",
    "        \"Equalized Odds\": equalized_odds(y_test_clean, y_pred_tree_train_clean_test_clean, s_test_clean),\n",
    "        \"Disparate Impact Ratio\": disparate_impact_ratio(y_pred_tree_train_clean_test_clean, s_test_clean),\n",
    "        \"Statistical Parity Difference\": statistical_parity_difference(y_pred_tree_train_clean_test_clean, s_test_clean)\n",
    "    },\n",
    "    \"Clean/Dirty\": {\n",
    "        \"Demographic Parity\": demographic_parity(y_pred_tree_train_clean_test_dirty, s_test_dirty),\n",
    "        \"Equalized Odds\": equalized_odds(y_test_dirty, y_pred_tree_train_clean_test_dirty, s_test_dirty),\n",
    "        \"Disparate Impact Ratio\": disparate_impact_ratio(y_pred_tree_train_clean_test_dirty, s_test_dirty),\n",
    "        \"Statistical Parity Difference\": statistical_parity_difference(y_pred_tree_train_clean_test_dirty, s_test_dirty)\n",
    "    },\n",
    "    \"Dirty/Clean\": {\n",
    "        \"Demographic Parity\": demographic_parity(y_pred_tree_train_dirty_test_clean, s_test_clean),\n",
    "        \"Equalized Odds\": equalized_odds(y_test_clean, y_pred_tree_train_dirty_test_clean, s_test_clean),\n",
    "        \"Disparate Impact Ratio\": disparate_impact_ratio(y_pred_tree_train_dirty_test_clean, s_test_clean),\n",
    "        \"Statistical Parity Difference\": statistical_parity_difference(y_pred_tree_train_dirty_test_clean, s_test_clean)\n",
    "    },\n",
    "    \"Dirty/Dirty\": {\n",
    "        \"Demographic Parity\": demographic_parity(y_pred_tree_train_dirty_test_dirty, s_test_dirty),\n",
    "        \"Equalized Odds\": equalized_odds(y_test_dirty, y_pred_tree_train_dirty_test_dirty, s_test_dirty),\n",
    "        \"Disparate Impact Ratio\": disparate_impact_ratio(y_pred_tree_train_dirty_test_dirty, s_test_dirty),\n",
    "        \"Statistical Parity Difference\": statistical_parity_difference(y_pred_tree_train_dirty_test_dirty, s_test_dirty)\n",
    "    }\n",
    "}\n",
    "\n",
    "col_widths = {\n",
    "    \"Train/Test\": 15,\n",
    "    \"Fairness Metric\": 30\n",
    "}\n",
    "\n",
    "# Header\n",
    "print(f\"{'Train/Test':<{col_widths['Train/Test']}}\"\n",
    "      f\"{'Fairness Metric':<{col_widths['Fairness Metric']}}\"\n",
    "      f\"Value\")\n",
    "print(\"-\" * (col_widths['Train/Test'] + col_widths['Fairness Metric'] + 96))\n",
    "\n",
    "def format_value(val):\n",
    "    if isinstance(val, float):\n",
    "        return f\"{val:.4f}\"\n",
    "    elif isinstance(val, dict):\n",
    "        return \"{\" + \", \".join(f\"{k}: {v:.4f}\" if isinstance(v, float) else f\"{k}: {v}\" for k, v in val.items()) + \"}\"\n",
    "    elif isinstance(val, tuple):\n",
    "        # Handle tuple of dicts or other types\n",
    "        return \"(\" + \", \".join(\n",
    "            \"{\" + \", \".join(f\"{k}: {v:.4f}\" if isinstance(v, float) else f\"{k}: {v}\" for k, v in d.items()) + \"}\" \n",
    "            if isinstance(d, dict) else \n",
    "            (f\"{d:.4f}\" if isinstance(d, float) else str(d))\n",
    "            for d in val\n",
    "        ) + \")\"\n",
    "    else:\n",
    "        return str(val)\n",
    "\n",
    "# Rows\n",
    "for config, metrics in results.items():\n",
    "    for metric_name, metric_value in metrics.items():\n",
    "        formatted = format_value(metric_value)\n",
    "        print(f\"{config:<{col_widths['Train/Test']}}\"\n",
    "              f\"{metric_name:<{col_widths['Fairness Metric']}}\"\n",
    "              f\"{formatted}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3ec752f5-bf02-4996-ada5-c3d32f0cacd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train/Test     Fairness Metric               Value\n",
      "---------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Clean/Clean    Demographic Parity            {1: 0.9519, 9: 0.7531, 2: 0.8904, 8: 0.6400, 6: 0.2769, 7: 1.0000, 3: 1.0000, 5: 0.5000}\n",
      "Clean/Clean    Equalized Odds                ({1: 1.0000, 9: 1.0000, 2: 1.0000, 8: 1.0000, 6: 1.0000, 7: 1.0000, 3: 1.0000, 5: 1.0000}, {1: 0.0000, 9: 0.0000, 2: 0.0000, 8: 0.0000, 6: 0.0000, 7: 0, 3: 0, 5: 0.0000})\n",
      "Clean/Clean    Disparate Impact Ratio        0.2769\n",
      "Clean/Clean    Statistical Parity Difference 0.7231\n",
      "Clean/Dirty    Demographic Parity            {3: 0.8889, 1: 0.9390, 9: 0.7778, 2: 0.8354, 8: 0.7018, 6: 0.3889, 7: 0.7500, 5: 0.7500, 4: 0.3750}\n",
      "Clean/Dirty    Equalized Odds                ({3: 1.0000, 1: 0.9719, 9: 0.9231, 2: 0.9683, 8: 0.9500, 6: 0.9444, 7: 1.0000, 5: 1.0000, 4: 0.6000}, {3: 0.0000, 1: 0.4333, 9: 0.1875, 2: 0.3125, 8: 0.1176, 6: 0.1111, 7: 0.3333, 5: 0.4000, 4: 0.0000})\n",
      "Clean/Dirty    Disparate Impact Ratio        0.3994\n",
      "Clean/Dirty    Statistical Parity Difference 0.5640\n",
      "Dirty/Clean    Demographic Parity            {1: 0.9462, 9: 0.7284, 2: 0.8904, 8: 0.6400, 6: 0.2923, 7: 1.0000, 3: 1.0000, 5: 0.5000}\n",
      "Dirty/Clean    Equalized Odds                ({1: 0.9939, 9: 0.9508, 2: 1.0000, 8: 0.9688, 6: 1.0000, 7: 1.0000, 3: 1.0000, 5: 1.0000}, {1: 0.0000, 9: 0.0500, 2: 0.0000, 8: 0.0556, 6: 0.0213, 7: 0, 3: 0, 5: 0.0000})\n",
      "Dirty/Clean    Disparate Impact Ratio        0.2923\n",
      "Dirty/Clean    Statistical Parity Difference 0.7077\n",
      "Dirty/Dirty    Demographic Parity            {3: 0.8889, 1: 0.9512, 9: 0.7654, 2: 0.8734, 8: 0.7018, 6: 0.3704, 7: 0.7500, 5: 0.7500, 4: 0.3750}\n",
      "Dirty/Dirty    Equalized Odds                ({3: 1.0000, 1: 0.9848, 9: 0.9077, 2: 0.9841, 8: 0.9500, 6: 0.9444, 7: 1.0000, 5: 1.0000, 4: 0.6000}, {3: 0.0000, 1: 0.4333, 9: 0.1875, 2: 0.4375, 8: 0.1176, 6: 0.0833, 7: 0.3333, 5: 0.4000, 4: 0.0000})\n",
      "Dirty/Dirty    Disparate Impact Ratio        0.3894\n",
      "Dirty/Dirty    Statistical Parity Difference 0.5808\n"
     ]
    }
   ],
   "source": [
    "# Fairness Metrics for Random Forests\n",
    "\n",
    "results = {\n",
    "    \"Clean/Clean\": {\n",
    "        \"Demographic Parity\": demographic_parity(y_pred_rf_train_clean_test_clean, s_test_clean),\n",
    "        \"Equalized Odds\": equalized_odds(y_test_clean, y_pred_rf_train_clean_test_clean, s_test_clean),\n",
    "        \"Disparate Impact Ratio\": disparate_impact_ratio(y_pred_rf_train_clean_test_clean, s_test_clean),\n",
    "        \"Statistical Parity Difference\": statistical_parity_difference(y_pred_rf_train_clean_test_clean, s_test_clean)\n",
    "    },\n",
    "    \"Clean/Dirty\": {\n",
    "        \"Demographic Parity\": demographic_parity(y_pred_rf_train_clean_test_dirty, s_test_dirty),\n",
    "        \"Equalized Odds\": equalized_odds(y_test_dirty, y_pred_rf_train_clean_test_dirty, s_test_dirty),\n",
    "        \"Disparate Impact Ratio\": disparate_impact_ratio(y_pred_rf_train_clean_test_dirty, s_test_dirty),\n",
    "        \"Statistical Parity Difference\": statistical_parity_difference(y_pred_rf_train_clean_test_dirty, s_test_dirty)\n",
    "    },\n",
    "    \"Dirty/Clean\": {\n",
    "        \"Demographic Parity\": demographic_parity(y_pred_rf_train_dirty_test_clean, s_test_clean),\n",
    "        \"Equalized Odds\": equalized_odds(y_test_clean, y_pred_rf_train_dirty_test_clean, s_test_clean),\n",
    "        \"Disparate Impact Ratio\": disparate_impact_ratio(y_pred_rf_train_dirty_test_clean, s_test_clean),\n",
    "        \"Statistical Parity Difference\": statistical_parity_difference(y_pred_rf_train_dirty_test_clean, s_test_clean)\n",
    "    },\n",
    "    \"Dirty/Dirty\": {\n",
    "        \"Demographic Parity\": demographic_parity(y_pred_rf_train_dirty_test_dirty, s_test_dirty),\n",
    "        \"Equalized Odds\": equalized_odds(y_test_dirty, y_pred_rf_train_dirty_test_dirty, s_test_dirty),\n",
    "        \"Disparate Impact Ratio\": disparate_impact_ratio(y_pred_rf_train_dirty_test_dirty, s_test_dirty),\n",
    "        \"Statistical Parity Difference\": statistical_parity_difference(y_pred_rf_train_dirty_test_dirty, s_test_dirty)\n",
    "    }\n",
    "}\n",
    "\n",
    "col_widths = {\n",
    "    \"Train/Test\": 15,\n",
    "    \"Fairness Metric\": 30\n",
    "}\n",
    "\n",
    "# Header\n",
    "print(f\"{'Train/Test':<{col_widths['Train/Test']}}\"\n",
    "      f\"{'Fairness Metric':<{col_widths['Fairness Metric']}}\"\n",
    "      f\"Value\")\n",
    "print(\"-\" * (col_widths['Train/Test'] + col_widths['Fairness Metric'] + 96))\n",
    "\n",
    "def format_value(val):\n",
    "    if isinstance(val, float):\n",
    "        return f\"{val:.4f}\"\n",
    "    elif isinstance(val, dict):\n",
    "        return \"{\" + \", \".join(f\"{k}: {v:.4f}\" if isinstance(v, float) else f\"{k}: {v}\" for k, v in val.items()) + \"}\"\n",
    "    elif isinstance(val, tuple):\n",
    "        # Handle tuple of dicts or other types\n",
    "        return \"(\" + \", \".join(\n",
    "            \"{\" + \", \".join(f\"{k}: {v:.4f}\" if isinstance(v, float) else f\"{k}: {v}\" for k, v in d.items()) + \"}\" \n",
    "            if isinstance(d, dict) else \n",
    "            (f\"{d:.4f}\" if isinstance(d, float) else str(d))\n",
    "            for d in val\n",
    "        ) + \")\"\n",
    "    else:\n",
    "        return str(val)\n",
    "\n",
    "# Rows\n",
    "for config, metrics in results.items():\n",
    "    for metric_name, metric_value in metrics.items():\n",
    "        formatted = format_value(metric_value)\n",
    "        print(f\"{config:<{col_widths['Train/Test']}}\"\n",
    "              f\"{metric_name:<{col_widths['Fairness Metric']}}\"\n",
    "              f\"{formatted}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d7e723c3-6276-4661-b2ff-94668f639363",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train/Test     Fairness Metric               Value\n",
      "---------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Clean/Clean    Demographic Parity            {1: 1.0000, 9: 1.0000, 2: 1.0000, 8: 1.0000, 6: 1.0000, 7: 1.0000, 3: 1.0000, 5: 1.0000}\n",
      "Clean/Clean    Equalized Odds                ({1: 1.0000, 9: 1.0000, 2: 1.0000, 8: 1.0000, 6: 1.0000, 7: 1.0000, 3: 1.0000, 5: 1.0000}, {1: 1.0000, 9: 1.0000, 2: 1.0000, 8: 1.0000, 6: 1.0000, 7: 0, 3: 0, 5: 1.0000})\n",
      "Clean/Clean    Disparate Impact Ratio        1.0000\n",
      "Clean/Clean    Statistical Parity Difference 0.0000\n",
      "Clean/Dirty    Demographic Parity            {3: 1.0000, 1: 1.0000, 9: 1.0000, 2: 1.0000, 8: 1.0000, 6: 1.0000, 7: 1.0000, 5: 1.0000, 4: 1.0000}\n",
      "Clean/Dirty    Equalized Odds                ({3: 1.0000, 1: 1.0000, 9: 1.0000, 2: 1.0000, 8: 1.0000, 6: 1.0000, 7: 1.0000, 5: 1.0000, 4: 1.0000}, {3: 1.0000, 1: 1.0000, 9: 1.0000, 2: 1.0000, 8: 1.0000, 6: 1.0000, 7: 1.0000, 5: 1.0000, 4: 1.0000})\n",
      "Clean/Dirty    Disparate Impact Ratio        1.0000\n",
      "Clean/Dirty    Statistical Parity Difference 0.0000\n",
      "Dirty/Clean    Demographic Parity            {1: 1.0000, 9: 1.0000, 2: 1.0000, 8: 1.0000, 6: 1.0000, 7: 1.0000, 3: 1.0000, 5: 1.0000}\n",
      "Dirty/Clean    Equalized Odds                ({1: 1.0000, 9: 1.0000, 2: 1.0000, 8: 1.0000, 6: 1.0000, 7: 1.0000, 3: 1.0000, 5: 1.0000}, {1: 1.0000, 9: 1.0000, 2: 1.0000, 8: 1.0000, 6: 1.0000, 7: 0, 3: 0, 5: 1.0000})\n",
      "Dirty/Clean    Disparate Impact Ratio        1.0000\n",
      "Dirty/Clean    Statistical Parity Difference 0.0000\n",
      "Dirty/Dirty    Demographic Parity            {3: 1.0000, 1: 1.0000, 9: 1.0000, 2: 1.0000, 8: 1.0000, 6: 1.0000, 7: 1.0000, 5: 1.0000, 4: 1.0000}\n",
      "Dirty/Dirty    Equalized Odds                ({3: 1.0000, 1: 1.0000, 9: 1.0000, 2: 1.0000, 8: 1.0000, 6: 1.0000, 7: 1.0000, 5: 1.0000, 4: 1.0000}, {3: 1.0000, 1: 1.0000, 9: 1.0000, 2: 1.0000, 8: 1.0000, 6: 1.0000, 7: 1.0000, 5: 1.0000, 4: 1.0000})\n",
      "Dirty/Dirty    Disparate Impact Ratio        1.0000\n",
      "Dirty/Dirty    Statistical Parity Difference 0.0000\n"
     ]
    }
   ],
   "source": [
    "# Fairness Metrics for SVC\n",
    "\n",
    "results = {\n",
    "    \"Clean/Clean\": {\n",
    "        \"Demographic Parity\": demographic_parity(y_pred_svm_train_clean_test_clean, s_test_clean),\n",
    "        \"Equalized Odds\": equalized_odds(y_test_clean, y_pred_svm_train_clean_test_clean, s_test_clean),\n",
    "        \"Disparate Impact Ratio\": disparate_impact_ratio(y_pred_svm_train_clean_test_clean, s_test_clean),\n",
    "        \"Statistical Parity Difference\": statistical_parity_difference(y_pred_svm_train_clean_test_clean, s_test_clean)\n",
    "    },\n",
    "    \"Clean/Dirty\": {\n",
    "        \"Demographic Parity\": demographic_parity(y_pred_svm_train_clean_test_dirty, s_test_dirty),\n",
    "        \"Equalized Odds\": equalized_odds(y_test_dirty, y_pred_svm_train_clean_test_dirty, s_test_dirty),\n",
    "        \"Disparate Impact Ratio\": disparate_impact_ratio(y_pred_svm_train_clean_test_dirty, s_test_dirty),\n",
    "        \"Statistical Parity Difference\": statistical_parity_difference(y_pred_svm_train_clean_test_dirty, s_test_dirty)\n",
    "    },\n",
    "    \"Dirty/Clean\": {\n",
    "        \"Demographic Parity\": demographic_parity(y_pred_svm_train_dirty_test_clean, s_test_clean),\n",
    "        \"Equalized Odds\": equalized_odds(y_test_clean, y_pred_svm_train_dirty_test_clean, s_test_clean),\n",
    "        \"Disparate Impact Ratio\": disparate_impact_ratio(y_pred_svm_train_dirty_test_clean, s_test_clean),\n",
    "        \"Statistical Parity Difference\": statistical_parity_difference(y_pred_svm_train_dirty_test_clean, s_test_clean)\n",
    "    },\n",
    "    \"Dirty/Dirty\": {\n",
    "        \"Demographic Parity\": demographic_parity(y_pred_svm_train_dirty_test_dirty, s_test_dirty),\n",
    "        \"Equalized Odds\": equalized_odds(y_test_dirty, y_pred_svm_train_dirty_test_dirty, s_test_dirty),\n",
    "        \"Disparate Impact Ratio\": disparate_impact_ratio(y_pred_svm_train_dirty_test_dirty, s_test_dirty),\n",
    "        \"Statistical Parity Difference\": statistical_parity_difference(y_pred_svm_train_dirty_test_dirty, s_test_dirty)\n",
    "    }\n",
    "}\n",
    "\n",
    "col_widths = {\n",
    "    \"Train/Test\": 15,\n",
    "    \"Fairness Metric\": 30\n",
    "}\n",
    "\n",
    "# Header\n",
    "print(f\"{'Train/Test':<{col_widths['Train/Test']}}\"\n",
    "      f\"{'Fairness Metric':<{col_widths['Fairness Metric']}}\"\n",
    "      f\"Value\")\n",
    "print(\"-\" * (col_widths['Train/Test'] + col_widths['Fairness Metric'] + 96))\n",
    "\n",
    "def format_value(val):\n",
    "    if isinstance(val, float):\n",
    "        return f\"{val:.4f}\"\n",
    "    elif isinstance(val, dict):\n",
    "        return \"{\" + \", \".join(f\"{k}: {v:.4f}\" if isinstance(v, float) else f\"{k}: {v}\" for k, v in val.items()) + \"}\"\n",
    "    elif isinstance(val, tuple):\n",
    "        # Handle tuple of dicts or other types\n",
    "        return \"(\" + \", \".join(\n",
    "            \"{\" + \", \".join(f\"{k}: {v:.4f}\" if isinstance(v, float) else f\"{k}: {v}\" for k, v in d.items()) + \"}\" \n",
    "            if isinstance(d, dict) else \n",
    "            (f\"{d:.4f}\" if isinstance(d, float) else str(d))\n",
    "            for d in val\n",
    "        ) + \")\"\n",
    "    else:\n",
    "        return str(val)\n",
    "\n",
    "# Rows\n",
    "for config, metrics in results.items():\n",
    "    for metric_name, metric_value in metrics.items():\n",
    "        formatted = format_value(metric_value)\n",
    "        print(f\"{config:<{col_widths['Train/Test']}}\"\n",
    "              f\"{metric_name:<{col_widths['Fairness Metric']}}\"\n",
    "              f\"{formatted}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d70e03b0-fc1c-40cd-9c5f-7719070eaed7",
   "metadata": {},
   "source": [
    "## Analysis of Results\n",
    "\n",
    "### Demographic Parity:\n",
    "In both datasets, prediction rates vary significantly across racial groups. In the clean data, groups 7 and 3 have 100% prediction rate while group 6 has only 28%. The dirty data shows less extreme variations, with rates between 44% and 93%.\n",
    "\n",
    "### Equalized Odds\n",
    "In the clean dataset, the true positive rates (TPR) are quite similar for all groups (around 1.0 for most groups), and false positive rates (FPR) are mostly 0. For the dirty dataset, the TPRs and FPRs deviate more between groups, indicating increased bias introduced by the data corruption.\n",
    "\n",
    "### Disparate Impact Ratio\n",
    "The clean dataset shows a disparate impact ratio of around 0.28, indicating significant disparity in prediction rates between the most and least favored groups. The dirty dataset shows an improved disparate impact ratio of around 0.48, but still reflects inequality.\n",
    "\n",
    "### Statistical Parity Difference\n",
    "The clean dataset has a parity difference of around 0.72, showing a noticeable discrepancy in positive prediction rates between different groups. The dirty dataset shows a smaller but still substantial disparity (0.49), suggesting that while the data corruption may slightly reduce the bias, it does not eliminate it.\n",
    "\n",
    "## Summary\n",
    "\n",
    "The dirty data (with functional dependency violations) significantly reduces model accuracy, but interestingly shows improved fairness metrics in some ways. Both datasets show substantial disparities across racial groups: the clean data has perfect classification accuracy but shows greater disparity between groups, the dirty data has lower accuracy but somewhat reduced disparities. Decision Trees are slightly more robust to the data quality issues than Logistic Regression. A potential explantaion for these results are that the functional dependency violations in the dirty data may have disrupted some of the patterns that led to the extreme disparities in the clean data, inadvertently reducing some fairness metrics while decreasing overall accuracy. This suggests an important trade-off between model accuracy and fairness metrics, highlighting the complex relationship between data quality and algorithmic fairness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "74feff6c-b5d6-4736-9932-ba7c411f0e28",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tree_clean' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 37\u001b[0m\n\u001b[1;32m     34\u001b[0m clean_feature_names \u001b[38;5;241m=\u001b[39m X_train_clean\u001b[38;5;241m.\u001b[39mcolumns\n\u001b[1;32m     35\u001b[0m dirty_feature_names \u001b[38;5;241m=\u001b[39m X_train_dirty\u001b[38;5;241m.\u001b[39mcolumns\n\u001b[0;32m---> 37\u001b[0m tree_clean_features \u001b[38;5;241m=\u001b[39m plot_feature_importance(\u001b[43mtree_clean\u001b[49m, clean_feature_names, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFeature Importance - Clean Data (Decision Tree)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     38\u001b[0m tree_dirty_features \u001b[38;5;241m=\u001b[39m plot_feature_importance(tree_dirty, dirty_feature_names, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFeature Importance - Dirty Data (Decision Tree)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     40\u001b[0m logreg_clean_features \u001b[38;5;241m=\u001b[39m plot_logistic_coefficients(logreg_clean, clean_feature_names, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFeature Coefficients - Clean Data (Logistic Regression)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tree_clean' is not defined"
     ]
    }
   ],
   "source": [
    "# For decision trees\n",
    "def plot_feature_importance(model, feature_names, title):\n",
    "    importances = model.feature_importances_\n",
    "    indices = np.argsort(importances)[::-1]\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.title(title)\n",
    "    plt.bar(range(len(indices)), importances[indices], align='center')\n",
    "    plt.xticks(range(len(indices)), [feature_names[i] for i in indices], rotation=90)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Return top 5 features and their importance\n",
    "    top_features = [(feature_names[i], importances[i]) for i in indices[:5]]\n",
    "    return top_features\n",
    "\n",
    "# For logistic regression\n",
    "def plot_logistic_coefficients(model, feature_names, title):\n",
    "    coef = model.coef_[0]\n",
    "    indices = np.argsort(np.abs(coef))[::-1]\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.title(title)\n",
    "    plt.bar(range(len(indices)), coef[indices], align='center')\n",
    "    plt.xticks(range(len(indices)), [feature_names[i] for i in indices], rotation=90)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Return top 5 features and their coefficients\n",
    "    top_features = [(feature_names[i], coef[i]) for i in indices[:5]]\n",
    "    return top_features\n",
    "\n",
    "# Use these to compare feature importance between clean and dirty data\n",
    "clean_feature_names = X_train_clean.columns\n",
    "dirty_feature_names = X_train_dirty.columns\n",
    "\n",
    "tree_clean_features = plot_feature_importance(tree_clean, clean_feature_names, \"Feature Importance - Clean Data (Decision Tree)\")\n",
    "tree_dirty_features = plot_feature_importance(tree_dirty, dirty_feature_names, \"Feature Importance - Dirty Data (Decision Tree)\")\n",
    "\n",
    "logreg_clean_features = plot_logistic_coefficients(logreg_clean, clean_feature_names, \"Feature Coefficients - Clean Data (Logistic Regression)\")\n",
    "logreg_dirty_features = plot_logistic_coefficients(logreg_dirty, dirty_feature_names, \"Feature Coefficients - Dirty Data (Logistic Regression)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4d1278-cc07-4c4f-96a6-743e787f24c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate fairness metrics for different prediction thresholds\n",
    "def fairness_by_threshold(model, X_test, y_test, sensitive_attr, thresholds=None):\n",
    "    if thresholds is None:\n",
    "        thresholds = np.linspace(0.1, 0.9, 9)\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for threshold in thresholds:\n",
    "        y_prob = model.predict_proba(X_test)[:, 1]\n",
    "        y_pred = (y_prob >= threshold).astype(int)\n",
    "        \n",
    "        dp = demographic_parity(y_pred, sensitive_attr)\n",
    "        tpr, fpr = equalized_odds(y_test, y_pred, sensitive_attr)\n",
    "        di = disparate_impact_ratio(y_pred, sensitive_attr)\n",
    "        spd = statistical_parity_difference(y_pred, sensitive_attr)\n",
    "        \n",
    "        # Calculate accuracy at this threshold\n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "        \n",
    "        results[threshold] = {\n",
    "            'accuracy': acc,\n",
    "            'demographic_parity': dp,\n",
    "            'equalized_odds_tpr': tpr,\n",
    "            'equalized_odds_fpr': fpr,\n",
    "            'disparate_impact': di,\n",
    "            'statistical_parity_diff': spd\n",
    "        }\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Apply to both models on both datasets\n",
    "logreg_clean_thresholds = fairness_by_threshold(logreg_clean, X_test_clean, y_test_clean, s_test_clean)\n",
    "logreg_dirty_thresholds = fairness_by_threshold(logreg_dirty, X_test_dirty, y_test_dirty, s_test_dirty)\n",
    "\n",
    "# Plot the trade-off between accuracy and fairness metrics\n",
    "thresholds = list(logreg_clean_thresholds.keys())\n",
    "clean_acc = [logreg_clean_thresholds[t]['accuracy'] for t in thresholds]\n",
    "clean_di = [logreg_clean_thresholds[t]['disparate_impact'] for t in thresholds]\n",
    "dirty_acc = [logreg_dirty_thresholds[t]['accuracy'] for t in thresholds]\n",
    "dirty_di = [logreg_dirty_thresholds[t]['disparate_impact'] for t in thresholds]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(thresholds, clean_acc, 'b-', label='Clean Data - Accuracy')\n",
    "plt.plot(thresholds, clean_di, 'b--', label='Clean Data - Disparate Impact')\n",
    "plt.plot(thresholds, dirty_acc, 'r-', label='Dirty Data - Accuracy')\n",
    "plt.plot(thresholds, dirty_di, 'r--', label='Dirty Data - Disparate Impact')\n",
    "plt.xlabel('Classification Threshold')\n",
    "plt.ylabel('Metric Value')\n",
    "plt.title('Accuracy vs. Fairness Trade-off')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a85474-b021-4353-ab18-0f2f16b7e0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to analyze model performance by group\n",
    "# 1: White alone, 2: Black or African American alone, 3: American Indian or Alaska Native alone, 4: Chinese alone, 5: Japanese alone\n",
    "# 6: Other Asian or Pacific Islander alone, 7: Other race alone, 8: Two or more races, 9: Asian Indian alone\n",
    "def analyze_by_group(y_true, y_pred, sensitive_attr):\n",
    "    groups = sensitive_attr.unique()\n",
    "    results = {}\n",
    "    \n",
    "    for group in groups:\n",
    "        mask = (sensitive_attr == group)\n",
    "        y_true_group = y_true[mask]\n",
    "        y_pred_group = y_pred[mask]\n",
    "        \n",
    "        # Skip groups with too few samples\n",
    "        if len(y_true_group) < 5:\n",
    "            continue\n",
    "        \n",
    "        results[group] = {\n",
    "            'accuracy': accuracy_score(y_true_group, y_pred_group),\n",
    "            'group_size': len(y_true_group),\n",
    "            'percent_positive_predictions': y_pred_group.mean() * 100,\n",
    "            'percent_positive_actual': y_true_group.mean() * 100,\n",
    "            'confusion_matrix': confusion_matrix(y_true_group, y_pred_group).tolist()\n",
    "        }\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Apply to both models on both datasets\n",
    "logreg_clean_by_group = analyze_by_group(y_test_clean, y_pred_clean, s_test_clean)\n",
    "logreg_dirty_by_group = analyze_by_group(y_test_dirty, y_pred_dirty, s_test_dirty)\n",
    "tree_clean_by_group = analyze_by_group(y_test_clean, y_pred_tree_clean, s_test_clean)\n",
    "tree_dirty_by_group = analyze_by_group(y_test_dirty, y_pred_tree_dirty, s_test_dirty)\n",
    "\n",
    "# Plot accuracy by group and dataset\n",
    "groups = sorted(set(logreg_clean_by_group.keys()) | set(logreg_dirty_by_group.keys()))\n",
    "clean_acc_by_group = [logreg_clean_by_group.get(g, {}).get('accuracy', 0) for g in groups]\n",
    "dirty_acc_by_group = [logreg_dirty_by_group.get(g, {}).get('accuracy', 0) for g in groups]\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "x = np.arange(len(groups))\n",
    "width = 0.35\n",
    "plt.bar(x - width/2, clean_acc_by_group, width, label='Clean Data')\n",
    "plt.bar(x + width/2, dirty_acc_by_group, width, label='Dirty Data')\n",
    "plt.xlabel('Race Group')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Model Accuracy by Race Group')\n",
    "plt.xticks(x, groups)\n",
    "plt.ylim(0, 1.05)\n",
    "plt.legend()\n",
    "plt.grid(True, axis='y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3609f1e1-b1dd-4caf-b158-abe032c09707",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to identify and analyze misclassified examples\n",
    "def analyze_misclassifications(X_test, y_test, y_pred, sensitive_attr):\n",
    "    # Find misclassified examples\n",
    "    misclassified = (y_test != y_pred)\n",
    "    X_misclassified = X_test[misclassified]\n",
    "    y_true_misclassified = y_test[misclassified]\n",
    "    s_misclassified = sensitive_attr[misclassified]\n",
    "    \n",
    "    # Count misclassifications by group\n",
    "    group_counts = s_misclassified.value_counts().to_dict()\n",
    "    total_by_group = sensitive_attr.value_counts().to_dict()\n",
    "    \n",
    "    # Calculate error rates by group\n",
    "    error_rates = {g: group_counts.get(g, 0) / total_by_group.get(g, 1) for g in total_by_group}\n",
    "    \n",
    "    # Analyze feature distributions of misclassified examples\n",
    "    misclassified_stats = X_misclassified.describe()\n",
    "    overall_stats = X_test.describe()\n",
    "    \n",
    "    return {\n",
    "        'total_misclassified': misclassified.sum(),\n",
    "        'error_rate': misclassified.mean(),\n",
    "        'group_error_counts': group_counts,\n",
    "        'group_error_rates': error_rates,\n",
    "        'misclassified_feature_stats': misclassified_stats,\n",
    "        'overall_feature_stats': overall_stats\n",
    "    }\n",
    "\n",
    "# Apply to both models on both datasets\n",
    "logreg_clean_errors = analyze_misclassifications(X_test_clean, y_test_clean, y_pred_clean, s_test_clean)\n",
    "logreg_dirty_errors = analyze_misclassifications(X_test_dirty, y_test_dirty, y_pred_dirty, s_test_dirty)\n",
    "tree_clean_errors = analyze_misclassifications(X_test_clean, y_test_clean, y_pred_tree_clean, s_test_clean)\n",
    "tree_dirty_errors = analyze_misclassifications(X_test_dirty, y_test_dirty, y_pred_tree_dirty, s_test_dirty)\n",
    "\n",
    "# Visualize error rates by group\n",
    "groups = sorted(set(logreg_clean_errors['group_error_rates'].keys()) | \n",
    "                set(logreg_dirty_errors['group_error_rates'].keys()))\n",
    "clean_errors = [logreg_clean_errors['group_error_rates'].get(g, 0) for g in groups]\n",
    "dirty_errors = [logreg_dirty_errors['group_error_rates'].get(g, 0) for g in groups]\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "x = np.arange(len(groups))\n",
    "width = 0.35\n",
    "plt.bar(x - width/2, clean_errors, width, label='Clean Data')\n",
    "plt.bar(x + width/2, dirty_errors, width, label='Dirty Data')\n",
    "plt.xlabel('Race Group')\n",
    "plt.ylabel('Error Rate')\n",
    "plt.title('Error Rates by Race Group')\n",
    "plt.xticks(x, groups)\n",
    "plt.legend()\n",
    "plt.grid(True, axis='y')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
